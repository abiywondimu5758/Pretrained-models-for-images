{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i0PveUrGjLt-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device assuming a GPU is available\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Load pre-trained Faster R-CNN model and move it to the GPU\n",
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Define the image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "annFile = 'coco/instances_val2017.json'\n",
        "coco = COCO(annFile)\n",
        "\n",
        "# Get all category IDs and then names\n",
        "cat_ids = coco.getCatIds()\n",
        "cats = coco.loadCats(cat_ids)\n",
        "coco_instance_category_names = [cat['name'] for cat in cats]\n",
        "\n",
        "print(coco_instance_category_names)\n",
        "\n",
        "# Folder containing the uploaded images\n",
        "folder_path = 'sample_images'\n",
        "\n",
        "# Folder to save processed images. Create the folder if it doesn't exist.\n",
        "processed_folder = 'processed_images'\n",
        "os.makedirs(processed_folder, exist_ok=True)\n",
        "\n",
        "# List all supported image files in the folder\n",
        "supported_formats = ('.jpg', '.jpeg', '.png')\n",
        "image_files = [fname for fname in os.listdir(folder_path) if fname.lower().endswith(supported_formats)]\n",
        "\n",
        "# Loop through each image file in the folder\n",
        "for fname in image_files:\n",
        "    image_path = os.path.join(folder_path, fname)\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    img_tensor = transform(image).to(device)\n",
        "\n",
        "    # Run inference\n",
        "    with torch.no_grad():\n",
        "        predictions = model([img_tensor])\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
        "    ax.imshow(image)\n",
        "\n",
        "    boxes = predictions[0]['boxes']\n",
        "    scores = predictions[0]['scores']\n",
        "    labels = predictions[0]['labels']\n",
        "\n",
        "    # Set a confidence threshold for displaying detections\n",
        "    threshold = 0.5\n",
        "\n",
        "    for i, score in enumerate(scores):\n",
        "        if score > threshold:\n",
        "            box = boxes[i].cpu().numpy()\n",
        "            label_idx = labels[i].cpu().numpy()\n",
        "            label_name = coco_instance_category_names[label_idx]\n",
        "            x1, y1, x2, y2 = box\n",
        "            # Draw bounding box\n",
        "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            # Annotate with label and score\n",
        "            text = f'{label_name}: {score:.2f}'\n",
        "            ax.text(x1, y1, text, fontsize=12, color='yellow', backgroundcolor='black')\n",
        "\n",
        "    ax.set_title(f'Detections for {fname}')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the annotated image to the processed_images folder\n",
        "    save_path = os.path.join(processed_folder, f'processed_{fname}')\n",
        "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close(fig)  # Close the figure to free memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "uac50QnUjao9",
        "outputId": "0e023ab2-6a91-41b7-add4-9574d4b162dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:01<00:00, 155MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'coco/instances_val2017.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2e7ac8143ada>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mannFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'coco/instances_val2017.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcoco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Get all category IDs and then names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coco/instances_val2017.json'"
          ]
        }
      ]
    }
  ]
}