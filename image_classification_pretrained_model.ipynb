{
  "cells": [
    {
      "metadata": {
        "id": "qpiJj8ym0v0-"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "AoilhmYe1b5t",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "import os, re, time, json\n",
        "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
        "import numpy as np\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HuG_q_1jkaZ6"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ]
    },
    {
      "metadata": {
        "id": "v4ocPhg6J_xw"
      },
      "cell_type": "markdown",
      "source": [
        "- Define the batch size\n",
        "- Define the class (category) names"
      ]
    },
    {
      "metadata": {
        "id": "cCpkS9C_H7Tl",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-o96NnyJ_xx"
      },
      "cell_type": "markdown",
      "source": [
        "Define some functions that will help us to create some visualizations."
      ]
    },
    {
      "metadata": {
        "id": "CfFqJxrzoj5Q",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#Matplotlib config\n",
        "plt.rc('image', cmap='gray')\n",
        "plt.rc('grid', linewidth=0)\n",
        "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
        "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
        "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
        "plt.rc('text', color='a8151a')\n",
        "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
        "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
        "\n",
        "# utility to display a row of digits with their predictions\n",
        "def display_images(digits, predictions, labels, title):\n",
        "\n",
        "  n = 10\n",
        "\n",
        "  indexes = np.random.choice(len(predictions), size=n)\n",
        "  n_digits = digits[indexes]\n",
        "  n_predictions = predictions[indexes]\n",
        "  n_predictions = n_predictions.reshape((n,))\n",
        "  n_labels = labels[indexes]\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 4))\n",
        "  plt.title(title)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i+1)\n",
        "    class_index = n_predictions[i]\n",
        "\n",
        "    plt.xlabel(classes[class_index])\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(n_digits[i])\n",
        "\n",
        "# utility to display training and validation curves\n",
        "def plot_metrics(metric_name, title, ylim=5):\n",
        "  plt.title(title)\n",
        "  plt.ylim(0,ylim)\n",
        "  plt.plot(history.history[metric_name],color='blue',label=metric_name)\n",
        "  plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wPq4Sw5akosT"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading and Preprocessing Data\n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset has 32 x 32 RGB images belonging to 10 classes. We will load the dataset from Keras."
      ]
    },
    {
      "metadata": {
        "id": "E103YDdQ8NNq",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "(training_images, training_labels) , (validation_images, validation_labels) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "prd944ThNavt"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize Dataset\n",
        "\n",
        "Use the `display_image` to view some of the images and their class labels."
      ]
    },
    {
      "metadata": {
        "id": "UiokWTuKo88c",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "display_images(training_images, training_labels, training_labels, \"Training Data\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-q35q41KNfxH",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "display_images(validation_images, validation_labels, validation_labels, \"Training Data\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ltKfwrCVNuIu"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocess Dataset\n",
        "Here, we'll perform normalization on images in training and validation set.\n",
        "- We'll use the function [preprocess_input](https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py) from the ResNet50 model in Keras."
      ]
    },
    {
      "metadata": {
        "id": "JIxdiJVKArC6",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def preprocess_image_input(input_images):\n",
        "  input_images = input_images.astype('float32')\n",
        "  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n",
        "  return output_ims\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QOqjKzgAEU-Z",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "train_X = preprocess_image_input(training_images)\n",
        "valid_X = preprocess_image_input(validation_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2fooPL9Gkuox"
      },
      "cell_type": "markdown",
      "source": [
        "## Define the Network\n",
        "We will be performing transfer learning on **ResNet50** available in Keras.\n",
        "- We'll load pre-trained **imagenet weights** to the model.\n",
        "- We'll choose to retain all layers of **ResNet50** along with the final classification layers."
      ]
    },
    {
      "metadata": {
        "id": "56y8UNFQIVwj",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Feature Extraction is performed by ResNet50 pretrained on imagenet weights.\n",
        "Input size is 224 x 224.\n",
        "'''\n",
        "def feature_extractor(inputs):\n",
        "\n",
        "  feature_extractor = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')(inputs)\n",
        "  return feature_extractor\n",
        "\n",
        "\n",
        "'''\n",
        "Defines final dense layers and subsequent softmax layer for classification.\n",
        "'''\n",
        "def classifier(inputs):\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classification\")(x)\n",
        "    return x\n",
        "\n",
        "'''\n",
        "Since input image size is (32 x 32), first upsample the image by factor of (7x7) to transform it to (224 x 224)\n",
        "Connect the feature extraction and \"classifier\" layers to build the model.\n",
        "'''\n",
        "def final_model(inputs):\n",
        "\n",
        "    resize = tf.keras.layers.UpSampling2D(size=(7,7))(inputs)\n",
        "\n",
        "    resnet_feature_extractor = feature_extractor(resize)\n",
        "    classification_output = classifier(resnet_feature_extractor)\n",
        "\n",
        "    return classification_output\n",
        "\n",
        "'''\n",
        "Define the model and compile it.\n",
        "Use Stochastic Gradient Descent as the optimizer.\n",
        "Use Sparse Categorical CrossEntropy as the loss function.\n",
        "'''\n",
        "def define_compile_model():\n",
        "  inputs = tf.keras.layers.Input(shape=(32,32,3))\n",
        "\n",
        "  classification_output = final_model(inputs)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n",
        "\n",
        "  model.compile(optimizer='SGD',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = define_compile_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CuhDh8ao8VyB"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ]
    },
    {
      "metadata": {
        "id": "2K6RNDqtJ_xx",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "history = model.fit(train_X, training_labels, epochs=EPOCHS, validation_data = (valid_X, validation_labels), batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CYb5sAEmk4ut"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model\n",
        "\n",
        "Calculate the loss and accuracy metrics using the model's `.evaluate` function."
      ]
    },
    {
      "metadata": {
        "id": "io7Fuu-w3PZi",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(valid_X, validation_labels, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yml-phRfPeOj"
      },
      "cell_type": "markdown",
      "source": [
        "### Plot Loss and Accuracy Curves\n",
        "\n",
        "Plot the loss (in blue) and validation loss (in green)."
      ]
    },
    {
      "metadata": {
        "id": "b1ZMMJ6T921A",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "plot_metrics(\"loss\", \"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbnWIbeJJ_xx"
      },
      "cell_type": "markdown",
      "source": [
        "Plot the training accuracy (blue) as well as the validation accuracy (green)."
      ]
    },
    {
      "metadata": {
        "id": "P0YpFs3J99eO",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "plot_metrics(\"accuracy\", \"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9jFVovcUUVs1"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize predictions\n",
        "We can take a look at the predictions on the validation set."
      ]
    },
    {
      "metadata": {
        "id": "NIQAqkMV9adq",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "probabilities = model.predict(valid_X, batch_size=64)\n",
        "probabilities = np.argmax(probabilities, axis = 1)\n",
        "\n",
        "display_images(validation_images, probabilities, validation_labels, \"Bad predictions indicated in red.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}